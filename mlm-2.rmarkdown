---
title: MLM
format: revealjs
slide-number: true
editor: source
execute:
  echo: true
html:
  code-fold: true
  code-summary: Show the code
---



How do we model our repeated measures? 


```{r, eval = FALSE}
lm(change_score ~ group, data = df) # difference score

lm(t2 ~ t1 + group, data = df) # residualized change
```


Equivalent if assumed everyone starts the same. But does not easily scale to more than 2 occasions, among other pitfalls 



-----------------------------


![](chelsea.jpeg)



----------------


```{r}
#| code-fold: true

library(tidyverse)
library(tidyr)

set.seed(555) 
n <- 500    
df <- data.frame(
  group = c(rep("Control", n), rep("Tx", n)),
  t1 = c(rnorm(n, mean = 90, sd = 10), rnorm(n, mean = 50, sd = 10))
)

df$group <- as.factor(df$group)
df$t2 <- 20 + (0.8 * df$t1) + rnorm(2*n, mean=0, sd=5)

df_long <- df |> 
  mutate(df, id = row_number()) |> 
  pivot_longer(
    cols = c(t1, t2),   
    names_to = "wave",  
    values_to = "value",  
    names_prefix = "t"  )
```


--------



```{r}
library(glmmTMB)

mod.1 <- glmmTMB(value ~ 1+ (1 | id), data = df_long)

summary(mod.1)

```



------------

Level 1 $${Y}_{ti} = \beta_{0i}  + \varepsilon_{ti}$$

Level 2 $${\beta}_{0i} = \gamma_{00} + U_{0i}$$

$${U}_{0i} \sim\mathcal{N}(0, \tau_{00}^{2})$$ $${e}_{ti} \sim\mathcal{N}(0, \sigma^{2})$$



----------------


```{r}
#| code-fold: true
library(readr)
mlm <- read_csv("https://raw.githubusercontent.com/josh-jackson/longitudinal-2021/master/example.csv")
mlm 
```

```{r}
#| code-fold: true
# Function to sample people and their observations

library(tidyverse)
sample_n_of <- function(data, size, ...) {
  dots <- quos(...)
  
  group_ids <- data %>% 
    group_by(!!! dots) %>% 
    group_indices()
  
  sampled_groups <- sample(unique(group_ids), size)
  data %>% 
    filter(group_ids %in% sampled_groups)
}
```


------------------------------------------------------------------------



## Empty model

Level 1 $${Y}_{ti} = \beta_{0i}  + \varepsilon_{ti}$$

Level 2 $${\beta}_{0i} = \gamma_{00} + U_{0i}$$


$${U}_{0i} \sim\mathcal{N}(0, \tau_{00}^{2})$$ 

$${e}_{ti} \sim\mathcal{N}(0, \sigma^{2})$$

------------------------------------------------------------------------


```{r}
#| code-fold: true

set.seed(123)
mlm %>%
  sample_n_of(8, ID) %>% 
ggplot(aes(x = week, y = CON, group = ID)) + geom_point(aes(color = factor(ID))) + stat_smooth(aes(color = factor(ID)), method = "lm", formula=y~1, se = FALSE) + xlab("time") + ylab("Y") + theme(legend.position = "none")
```


$${Y}_{ti} = \gamma_{00} + U_{0i}  + \varepsilon_{ti}$$

Akin to ANOVA if we treat $U_{0i}$ as between subjects variance & $\varepsilon_{ti}$ as within subjects variance.

## MLM intuitions

Anytime you have repeated DVs you should use MLM as opposed to doing aggregation outside the model. While that should be your default, it is helpful to conceptualize why it is helpful.

1.  Aggregation is bad
2.  Regressions within regressions (ie coefficients as outcomes)
3.  Questions at different levels
4.  Variance decomposition
5.  Learning from other data through pooling/shrinkage


## Handling multiple DVs?

What if people had more than 1 DV, like we do with time? What do you do with multiple items, multiple trials, multiple _____?

Two options: 1. Collapse and average across.

## Example


```{r}
#| code-fold: true
library(tidyverse)
library(broom)

simp<- tribble(
  ~ID, ~group,  ~test.score, ~study,
1,1,5,1,
2,1,7,3,
3,2,4,2,
4,2,6,4,
5,3,3,3,
6,3,5,5,
7,4,2,4,
8,4,4,6,
9,5,1,5,
10,5,3,7)
```

```{r, echo=FALSE}
simp %>% 
ggplot(aes(x=study, y=test.score)) +
    geom_point(aes(colour = factor(group)))
```


------------------------------------------------------------------------

Could aggragate across group


```{r, echo = FALSE}
simp.1<- tribble(
  ~ID, ~group,  ~test.score, ~study,
  1,1,6,2,
  2,2,5,3,
  3,3,4,4,
  4,4,3,5,
  5,5,2,6)
```

```{r, echo=FALSE}
simp.1 %>% 
ggplot(aes(x=study, y=test.score)) +
    geom_point() +    
    geom_smooth(method=lm, se=FALSE) +
  geom_point(data = simp, aes(colour = factor(group)))
```


------------------------------------------------------------------------


```{r, echo = FALSE}
simp %>% 
ggplot(aes(x=study, y=test.score, group = group)) +
    geom_point(aes(colour = factor(group))) +   
    geom_smooth(aes(colour = factor(group)),method=lm,se=FALSE)
```



## 1. Aggregation obscures hypotheses

-   Between person H1: Do students who study more get better grades?

-   Within person H2: When a student studies, do they get better grades?

-   H1 and H2 are independent from one another! Aggregation collapses the two. When you have nested data with many DVs it is important to not aggregate.


## 2. Regressions within regressions

Helps to take multilevel and split it into the different levels.

Level 1 is the smallest unit of analysis (students, waves, trials, family members)

Level 2 variables are what level 1 variables are "nested" in (people, schools, counties, families, dyads)

We are going to use level one components to run a regression, all the while level 1 is also estimating a regression. (Coefficents as outcomes)


## Longitudinal example

Level 1 can be thought of as regressions for each person with time as a predictor. Time can be a simple vector of integers indicating wave, year, hour, etc. 

$$Y_\text{josh} = \beta_{0josh} + \beta_{1josh} + \varepsilon_\text{josh}$$

We are going to run regressions for each person in the dataset, N = 1. 

----------

- After we run regressions for everyone at level 1, we are going to summarize each person's line based on their starting value $\beta_{0i}$  and their slope $\beta_{1i}$

- We now have a vector of starting values (1 for each person) and a vector of slopes (one for each person).

- We are going to treat these vectors as DVs in level 2, and fit regressions. 

------------------------------------------------------------------------

L1: 
$$Y_{time, i} = \beta_{0i} + \beta_{1i}X_{time,i} + \varepsilon_{time,i}$$
L2: 

$$\beta_{0i} = \gamma_{00} + U_{0i}$$ 
$$\beta_{1i} = \gamma_{10} + U_{1i}$$

-------------


$$\beta_{0i} = \gamma_{00} + U_{0i}$$ 

- Each of the vectors from level 1 can be decomposed further by a simple regression. 

- The level 2 intercept ($\gamma_{00}$) can be thought of as the average effect, just like in our null "intercept only models"

- The residual ($U_{0i}$) serves as our way to allow people (or whatever grouping variable) to differ from the average. 

------------------------------------------------------------------------

- The logic applies to any level 1 variable that is turned into a level 2 DV. 

$$\beta_{1i} = \gamma_{10} + U_{1i}$$
- Here our slope is the dv, the intercept $\gamma_{10}$ reflects the average and the residual $U_{1i}$ reflects person level deviations. 

- We can treat this like any other regression, adding terms to test hypothesis that end up changing the value of other coefficients. 

$$\beta_{1i} = \gamma_{10} + \gamma_{11}EDU + U_{1i}$$

## Some reminders

- Any level 1 variable is automatically a level 2. Your choice if you want to keep with a single average (fixed effect) or if you also want to model a random effect, though we will "keep it maximal"

- Random effects are (typically) deviation scores, and thus need to be interpreted with dispersion estimates

- The interpretation of coefficients are dictated by regression 101 rules. Remember: centering, dummy/effect coding, links, etc. 


## 3. Different types of hypotheses

To sum up the first two ways to think about regression, we take a relationship that could be simplified by aggregation, and conduct regressions from regressions. (Regression inception).

The third way is to think of questions at different levels. One difficulty people have is mixing types of questions. 

At level 1 we can ask lower-unit questions e.g., if you are stressed at one time point, will you have higher scores?  Differentiates fluctuations vs level 2-long term change


------------------------------------------------------------------------

At level 2 we can ask broader-unit questions. E.g., are stressful life events associated with change 

Often level 2 is between person variables.

Both levels are simple regressions. Level 2 uses coefficients from level 1 as DVs. Level 1 variables are time varying, while level 2 variables are time invariant.

## 4. Variance decomposition

For standard regression, we think of error as existing in one big bucket called $\varepsilon$ . Everything that we do not know goes into that bucket, from measurement error to missing variables.

For MLMs we will be breaking up ( $\varepsilon$ ) into multiple buckets (Us; random/varying effects) that reflect person level deviations across.

If we are interested in individual differences in intraindividual change, we must have a model that is capable of person specific predictions.  

------------

- Most studies present fixed effect findings, focusing on the person average effect. Often we are going to be interested in deviations from the effect. We model these person specifics through our random effects. 

$$Y_{t i} = \beta_{0i} + \beta_{1i}X_{t i} + \varepsilon_{ti}$$

$$\beta_{0} = \gamma_{00} + U_{0i}$$ 
$$\beta_{1} = \gamma_{10} +\gamma_{11}Z_i+ U_{1i}$$

------------------------------------------------------------------------

![](btw.png)

Assume a simple pre-post null model with long data. Y ~ 1.   

------------------------------------------------------------------------

![](win.png)

A MLM intercept only model [y ~ 1 + (1 |ID)] takes what was previously chalked up to error and reassigns it into person specific "buckets"

------------------------------------------------------------------------


- We will treat random effects as variables themselves e.g. individual differences in change. They index how much people DIFFER on some effect. 

- We can relate the random effects to other random effects e.g., do people who increase faster also start higher? 

- We can make person specific predictions by feeding our regression different values (much like we create a regression line by feeding the equation different values of X). 


## 5. Shrinkage/partial pooling

-   We treat our group variable as coming from a population. All groups are alike (because they are from the same population), but different in their own way. Because of this it is helpful to use information from other groups to help make predictions.

-   We do this in standard regression where we make predictions based on values from the whole dataset (not just binned Xs). A similar analogy for MLM is running a regression for each group. We want to pool as this leads to better predictions as we are not over fitting our data!

------------------------------------------------------------------------

If we take an empty model

$$Y_{time, i} = \beta_{0i} +  \varepsilon_{time, i}$$

$$\beta_{0i} = \gamma_{00} + U_{0i}$$

Where does $U_{0i}$ come from? If we calculated each by hand, through taking the average assessment for a person i and subtracting that from the grand mean average $\gamma_{00}$, would that equal $U_{0i}$ ?

## Complete, partial and no pooling

-   Complete assumes everyone is the same, with $U_{0i}$ being zero for everyone.

-   No pooling is if we calculate every person's effect with a regression, subtracting out he grand mean average.

-   Partial pooling is in the middle, a weighted average between the two. For those with fewer observations there is less information for a particular individual, thus the complete pooling estimate will be given more weight. If someone has a lot of data, their weighted average is closer to no pooling.

-   Partial pooling prevents both over and under fitting of your data, leading to increased out of sample predictions.

## Complete pooling

Ignores any dependency. Doesn't learn from others, assumes everyone is the same. Underfits the model.


```{r}
#| code-fold: true

ggplot(mlm, aes(x = week, y = SMN7)) + geom_point() + stat_smooth(method="lm") +ylab("test score") + xlab("study") +  theme(axis.ticks.y = element_blank(),
        axis.text.y = element_blank())

```


## No Pooling

Everyone is unique and we cannot learn from others. Leads to overfitting


```{r}
#| code-fold: true

ggplot(mlm, aes(x = week, y = SMN7, group = ID)) + geom_line() +  geom_point() + facet_wrap( ~ ID) +ylab("test score") + xlab("study")

```


## Partial pooling aka shrinkage aka regularization


```{r}
#| code-fold: true

library(viridis)
library(marginaleffects)
library(lme4)
model <- lmer(SMN7 ~ week + (1 + week | ID), data = mlm)

avg_line <- predictions(model, newdata = datagrid(week = 1:6), 
                        re.form = NA)

predictions(model, newdata = datagrid(ID = unique(mlm$ID), week = 1:6), allow.new.levels = T) |> 
ggplot() +
  geom_line(aes(x = week, y = estimate, group = ID), 
            alpha = 0.3, color = "blue") + 
  geom_line(data = avg_line, 
            aes(x = week, y = estimate),
            size = 2, color = "black")
```


------------------------------------------------------------------------

Partial pooling aka shrinkage provides the optimal amount of learning from others. Assumes people come from the same distribution but are distinct from one another.

If you have a little data, then the safe bet is to look at the average. If you have a lot of data, you can ignore others.


------------------------


- Intermission



## Basic Longitudinal Models

- Null or intercept only model

Level 1: $${Y}_{ti} = \beta_{0i}  + \varepsilon_{ti}$$

Level 2: $${\beta}_{0i} = \gamma_{00} + U_{0i}$$ Combined: $${Y}_{ti} = \gamma_{00} + U_{0i}  + \varepsilon_{ti}$$



## ICC

Between version variance over total variance. If the ICC is greater than zero, we are breaking standard regression assumptions as this is an index of dependency.

$$\frac{U_{0i}}{U_{0i}+ \varepsilon_{ti}}$$

ICC can also be interpreted as the average (or expected) correlation within a nested group, in this case a person. On other words, the ICC is the correlation between any person's repeated measures (technically residuals).


## GSOEP EXAMPLE


```{r}
#| code-fold: true
library(plyr)
library(tidyverse)
library(tidybayes)
library(psych)
library(lme4)
library(brms)

codebook <- read.csv("https://raw.githubusercontent.com/josh-jackson/longitudinal-2021/master/codebook.csv")

codebook <- codebook %>% 
    mutate(old_name = str_to_lower(old_name))

old.names <- codebook$old_name # get old column names
new.names <- codebook$new_name # get new column names

soep <- read.csv("https://raw.githubusercontent.com/josh-jackson/longitudinal-2021/master/soepdata.csv")

 soep <-  soep %>% # read in data
  select(all_of(old.names)) %>% # select the columns from our codebook
  setNames(new.names) # rename columns with our new names


soep <- soep %>%
  mutate_all(~as.numeric(mapvalues(., from = seq(-1,-7, -1), to = rep(NA, 7), warn_missing = F)))

soep_long <- soep %>%
  pivot_longer(cols = c(-contains("Procedural"), -contains("Demographic")),
               names_to = "item", 
               values_to = "value", 
               values_drop_na = TRUE) %>%
  left_join(codebook %>% select(item = new_name, reverse, mini, maxi)) %>% 
  separate(item, c("type", "item"), sep = "__") %>% 
  separate(item, c("item", "year"), sep = "[.]") %>% 
  separate(item, c("trait", "item"), sep = "_") %>% 
  mutate(value = as.numeric(value), # change to numeric
         value = ifelse(reverse == -1, 
            reverse.code(-1, value, mini = mini, maxi = maxi), value))

b5_soep_long <- soep_long %>%
  filter(type == "Big 5") %>% 
  group_by(Procedural__SID, trait, year) %>% 
  dplyr::summarize(value = mean(value, na.rm = T)) %>% 
  ungroup() %>% 
  left_join(soep_long %>% 
    select(Procedural__SID, DOB = Demographic__DOB, Sex = Demographic__Sex) %>%
    distinct())


```

```{r, echo = TRUE}
str(b5_soep_long)
```


--------------



```{r}
#| code-fold: true
b5_long <- b5_soep_long %>% 
  pivot_wider(names_from = trait, values_from = value) %>% 
  mutate(ID = Procedural__SID) %>% 
  mutate(ID = as.factor(ID))

```

```{r}
b5_long <-b5_long %>% 
  group_by(ID) %>%
  filter(!n() == 1) %>% 
  ungroup(ID)
```

```{r, echo = TRUE}
str(b5_long)
```


------------------------------------------------------------------------


```{r}

mod.1 <- glmmTMB(C ~ 1 + (1 | ID), data=b5_long)
```


------------------------------------------------------------------------


```{r}
summary(mod.1)
```




------------------------------------------------------------------------


```{r}
library(parameters) 

model_parameters(mod.1)
```


------------------------------------------------------------------------

ICC By hand or


```{r}
library(performance) 
model_performance(mod.1)
```


## what do the random effects look like?


```{r}
#| code-fold: true
library(modelbased)
random <- estimate_grouplevel(mod.1) 
head(random)

```


------------------------------------------------------------------------


```{r}
#| code-fold: true
library(see)
random %>% 
sample_n(100) %>% 
plot(.) 
```



## Predicted scores

Predictors are super important for evaluating our model as well as graphing. Lots of packages have these capabilities. My favorites are tidybayes (for brms), modelr (for all), marginaleffects (for all), and modelbased (for all).

These extend the flexibility of predict (base) and similar functions from emmeans

------------------------------------------------------------------------


```{r, echo = TRUE}
library(modelbased)
estimate_prediction(mod.1) %>% 
  head() 
```



------------------------------------------------------------------------


```{r}
library(marginaleffects)

predictions(mod.1) %>%  head()
```




------


```{r}
predictions(mod.1, newdata = datagrid(ID = c(901, 2301)))
```




## Within person empty null model

$${Y}_{ti} = \gamma_{00} + U_{0i}  + \varepsilon_{ti}$$

This model is helpful in producing the simplest longitudinal model, one where it states: there is an average value $\gamma_{00}$ that people differ along $U_{0i}$ . Because time is not in the model it assumes people do not change. $\varepsilon_{ti}$ reflects variation around each person's predicted score ( $\gamma_{00} + U_{0i}$ ).





## Growth model

Growth model is just a fancy term for including TIME as our level 1 predictor where we are now creating lines for each person.


Level 1: 
$${Y}_{ti} = \beta_{0i}  + \beta_{1i}Time_{ti} + \varepsilon_{ti}$$ 
Because we have a level 1 time predictor in the model, we are now asking the question, how does time influence scores on our DV? Because all regressions are linear by default, it asks if there is an association such that as time increases does the DV? 


------------------------------------------------------------------------

Level 2 takes the parameters at level 1 and decomposes them into a fixed component ( $\gamma$ ) that reflects the average and, if desired, the individual deviations around that fixed effect (U).

level 1 $${Y}_{ti} = \beta_{0i}  + \beta_{1j}Time_{ti} + \varepsilon_{ti}$$

level 2 $${\beta}_{0i} = \gamma_{00} + U_{0i}$$\
$${\beta}_{1i} = \gamma_{10}$$


--------

Combined equation: $${Y}_{ti} = \gamma_{00} + \gamma_{10} (\text{time}_{1i})+ U_{0i}  + \varepsilon_{ti}$$

Combined with random slope: $${Y}_{ti} = \gamma_{00} + \gamma_{10}(time_{ti})+ U_{0i} + U_{1i}(time_{ti}) + \varepsilon_{ti}$$

Can think of a persons score divided up into a fixed component as well as the random component.

$${\beta}_{1.6} = \gamma_{10} \pm U_{1.6}$$

---------

Notice how if we constrain random effects this just turns into a simple regression

level 1 $${Y}_{ti} = \beta_{0i}  + \beta_{1i}\text{time}_{ti} + \varepsilon_{ti}$$

level 2 $${\beta}_{0i} = \gamma_{00} $$\

$${\beta}_{1i} = \gamma_{10}$$


## The role of time

- If we change the time variable, we change the interpretation of the growth curve (age vs time in study).

- As with all regressions, zero is important. 

- Our time variable can be categorical or continuous, most often it is centered around some meaningful time (initial status, average age, etc). 



## Growth curve example


```{r}
#| code-fold: true
mod.2 <- glmmTMB(C ~ 1 + year + (1 | ID), data=b5_long)
summary(mod.2)
```


------------------------------------------------------------------------


```{r}
#| code-fold: true
b5_long<- b5_long %>%  
  mutate(year = as.numeric(year))
  
mod.3 <- lmer(C ~ 1 + year + (1 | ID), data=b5_long)
summary(mod.3)

```


------------------------------------------------------------------------

### The importance of scaling time


```{r}
#| code-fold: true
b5_long<- b5_long %>%  
  mutate(year.c = (year - 2005))
  
  
mod.4 <- lmer(C ~ 1 + year.c + (1 | ID), data=b5_long)
summary(mod.4)

```


## Visualizing results (quickly)

There are many ways to do this. The parameters package paired with the see package, both from the easystats package, are useful in this regard.


```{r}
#| code-fold: true
library(parameters)
library(see)

result <- model_parameters(mod.4)
result
```


------------------------------------------------------------------------


```{r}
plot(result)
```


------------------------------------------------------------------------


```{r}
#| code-fold: true
result2 <- simulate_parameters(mod.4)
plot(result2, show_intercept = FALSE)
```




## Feed our model

-   These graphs however just take your summary output and make it look nice. 

- If we want to say, plot the predicted line we need to do an additional step and FEED the data back into the model. Same as with graphing simple regressions

- The logic is simple: 1. fit model. 2. create grid that defines points to plot 3. feed grid into model to create predictions

------------------------------------------------------------------------

Rewriting slope equation to highlight what simple slope we want to graph. Just have to choose what we want to fix and what we want to vary

$$\hat{Y}_{ti} = [\gamma_{00}] + [\gamma_{10}   ]  * Time_{ti}$$ No random effects, because we wanted the expected value of the sample (or fixed effect) estimate graphed

------------------------------------------------------------------------

Some packages do so much behind the scenes it is hard to know what is happening


```{r}
#| code-fold: true
library(ggeffects)
p.mod4<-ggpredict(mod.4, "year.c")
p.mod4
```


Automatically chooses levels \[0,4,8\]

------------------------------------------------------------------------


```{r}
plot(p.mod4)
```


------------------------------------------------------------------------

If you want you can also get predicted lines for a particular individual. I really like the package marignaleffects as it is more flexible than ggeffects


```{r}
#| code-fold: true
library(marginaleffects)
p.r.mod2 <- predictions(mod.4, 
          newdata = 
          datagrid(ID = unique, 
                   year.c = 0:8))

p.r.mod2

```



$$\hat{Y}_{ti} = [\gamma_{00} + U_{0i}] + [\gamma_{10}+U_{1i} ]  * Time_{ti}$$

------------------------------------------------------------------------


```{r}
#| code-fold: true
p.r.mod2 |> 
sample_n_of(74, ID) |> 
ggplot(aes(year.c, estimate, level = ID)) +
  geom_line() 
```



-----------


```{r}
#| code-fold: true
predictions(mod.4, 
          newdata = 
          datagrid(year.c = 0:8), 
                   re.form = NA) |> 
  ggplot(aes(x = year.c, y = estimate, ymin = conf.low, ymax = conf.high)) +
  geom_ribbon(alpha = 0.1, fill = "red") +
  geom_line()

```





## MLM as default and make it maximum

If you don't include all random effects then you are losing information (GEE being a potential exception).

The question often is necessitated by model convergence. There are multiple opinions, but mine is to start maximum, and then work downward (ie simplify) if necessary/consistent with theory. It doesn't really hurt to include them, especially with new estimation advances (Bayesian estimation)



## Error Structure

The residual structure, where the random effects are again normally distributed with a mean of zero, but this time one must also consider covariance in addition to variance.

G matrix (books term, some call tau matrix) $$\begin{pmatrix} {U}_{0j} \\ {U}_{1j} \end{pmatrix}
\sim \mathcal{N} \begin{pmatrix} 
  0,      \tau_{00}^{2} & \tau_{01}\\ 
  0,  \tau_{01} & \tau_{10}^{2}
\end{pmatrix}$$

----------

$${e}_{ij} \sim \mathcal{N}(0, \sigma^{2})$$ The ${e}_{ij}$ is structured as a NxN matrix (R matrix), where n reflect number of waves. Usually assumed uncorrelated with equal variance across observations

Indexes within person variation

## Decomposing variance for random intercept model

V matrix = R + G matrices (technically $V_i = Z_iG_iZ{_i}^T + R_i$ where Z = N waves x number of random effects) 

Typical assumption is whats called compound symmetry. For an intercept only model the reason waves are correlated are only because of the random effect differences in means. 

 $$\text{Total variance CS} = \begin{pmatrix} 
       \tau_{00}^{2} + \sigma^{2}& \tau_{00}^{2} & \tau_{00}^{2}\\ 
       \tau_{00}^{2} &  \tau_{00}^{2} + \sigma^{2} &  \tau_{00}^{2}\\
       \tau_{00}^{2} & \tau_{00}^{2} &   \tau_{00}^{2} + \sigma^{2}
\end{pmatrix}$$

If people change at different rates, the variances and covariances over time have to change, too. 

## Error assumptions

Level 1 residuals are independent for Level 1 units across people\
AND

Level 1 residuals are independent of random effects

AND

Level 1 residuals are the same magnitude across people

We can modify a standard assumption: Level 1 residuals are independent within a person through different variance/covariance structures

-------------

If slopes are different across people, then people differ from each other systematically in 2 ways (U0i and U1i) then compound symmetry will NOT hold

-----------

$$\mathbf{V}_i = 
\begin{bmatrix} 
1 & 0 \\ 
1 & 1 \\ 
1 & 2 \\ 
1 & 3 
\end{bmatrix}
\begin{bmatrix} 
\tau_{U_0}^2 & \tau_{U_{01}} \\ 
\tau_{U_{01}} & \tau_{U_1}^2 
\end{bmatrix}
\begin{bmatrix} 
1 & 1 & 1 & 1 \\ 
0 & 1 & 2 & 3 
\end{bmatrix} + 
\begin{bmatrix} 
\sigma_e^2 & 0 & 0 & 0 \\ 
0 & \sigma_e^2 & 0 & 0 \\ 
0 & 0 & \sigma_e^2 & 0 \\ 
0 & 0 & 0 & \sigma_e^2 
\end{bmatrix}$$


------------

V matrix is time dependent. 

Predicted Time-Specific Variance = 
$$V_i = \tau_{U_0}^2 + \left[ (\text{time})^2 \tau_{U_1}^2 \right] + \left[ 2(\text{time})\tau_{U_{01}} \right] + \sigma_e^2$$

This is because as two people's slopes differ from one another the outcome variance and covariance must differ over time

------------

Two ends of an extreme, similar to no and full pooling. 

- Compound Symmetry (CS) is parsimonious but likely not correct. 
- Unstructured, where each wave variance and covariance are independent estimated provides a good fit, but likely overfit and involves estimating more parameters. 

- Random effects V matrix is somewhere, optimally, in between! 



## Revisit 2 wave assessment

Where is our error for a 2 wave model and: 

- Assuming random slopes 
- Assuming no random slopes


## Centering

Because mlms are regressions, and because mlms involve interactions, it is important to consider how your predictors zero point is defined.

How do you want your intercept interpreted? How do you want lower order terms in an interaction interpreted?

We will use these extensively to help disentangle within and between person variance.

## Uncentered

The default will give you predicted score of intercept when all predictors are zero.

Because most models will have a random intercept, it is important to keep in mind interpretations as we will be looking at variations around this value.



## Grand mean Centered

Zero now represents that grand mean of the sample. Calculated by taking $x_{ti} - \bar{x}$

Useful as this is often our the default in other methods. Changes meaning of intercept but not slope.

A related way to center is group grand mean centering where you take the mean of your grouping variables rather than the grand mean.

## group mean centering (person centering)

Calculated by taking $x_{ti} - \bar{x_i}$

Can change meaning of intercept and slope. Intercept is now a person's average level rather than the samples average level (grand mean) and level when predictors = 0 (no centering)

Slope at level 1 is the expected change relative to a person's average.

## other centering approaches

- can also center around specific events eg time until death or time before and after graduation where graduation is now coded as zero

## 0 as starting


```{r}
#| code-fold: true
# grand mean centering

b5_long<- b5_long %>%  
  mutate(year.c = (year - 2005))
  
  
mod.6 <- lmer(C ~ 1 + year.c + (1 + year.c  | ID), data=b5_long)

summary(mod.6)
```


## 0 as mid


```{r}
#| code-fold: true
b5_long<- b5_long %>%  
  mutate(year.cM = (year - 2009))
  
  
mod.7 <- lmer(C ~ 1 + year.cM + (1 + year.cM | ID), data=b5_long)

summary(mod.7)

```



## Smushing variance

Each level-1 predictor that is NOT TIME is really 2 predictor variables. It is important to separate within-person from between person variance. Failing to do so will "smush" between and within variance to level 1.


------------------------------------------------------------------------

Level 1: $${Y}_{ij} = \beta_{0j}  + \beta_{1j}(X_{ij}- \bar{X_j}) + \varepsilon_{ij}$$

Level 2: $${\beta}_{0j} = \gamma_{00} + \bar{X_j} +   U_{0j}$$

$${\beta}_{1j} = \gamma_{10} + \bar{X_j} + U_{1j}$$

-------------

- Time is a special level-1 variable, as time unfolds similarly for all, so we do not necessarily have to center it nor add back in the average. 

- Because time unfolds the same for everyone, there are no between people differences. Thus no need to center.  

- Sometimes we will center to reduce slope correlation with intercept. Typically we "center" to get a meaningful zero. 



## Estimation

We need to identify: 1. the estimates of each parameter 2. some measure of precision of that estimate (SEs) 3. an index of overall model fit (deviance/-2LL/aic/bic)

We will use maximum likelihood (and variants of) as well as MCMC (Bayesian) for estimation.

Model comparison is usually done through a likelihood ratio test distributed as a chi square.

## ML vs REML

REML = Restricted maximum likelihood

Similar to sample vs population estimates of SD where we do or don't divide by n-1, ML downward biases random effect estimates.

REML maximizes the likelihood of the residuals, so models with different fixed effects are not on the same scale and are not comparable. As a result, you cannot compare fixed models with likleihood metrics (aic) with REML. You can compare variance differences.

## Testing significance 

Methods for testing single parameters
From worst to best:

1. Wald Z-tests. Easy to compute. However, they are asymptotic approximations, assuming both that (1) the sampling distributions of the parameters are multivariate normal and that (2) the sampling distribution of the log-likelihood is (proportional to) Ï‡2.
2. Wald t-tests
3. Likelihood ratio test.  
4. Markov chain Monte Carlo (MCMC) or parametric bootstrap confidence intervals

## Likelhiood ratio test
How much more likely the data is under a more complex model than under the simpler model (these models need to be nested to compare this).

Log Likelihood (LL) is derived from ML estimation. Larger the LL the better the fit. Deviance compares two LLs. Current model and a saturated model (that fits data perfectly). 

Deviance = -2[LL current - LL saturated]

LL saturated = 1 for MLMs (probability it will perfectly recapture data). log of 1 is 0. So this term drops out. 

Deviance = -2LL current model. 

## Likelhiood ratio test

Comparing 2 models is called a likelihood ration test. Need to have: 
1. same data
2. nested models (think of constraining a parameter to zero)

Distributed as chi-square with df equal to constraint differences between models. 


```{r}
#| code-fold: true
anova(mod.2, mod.1)
```




## correlations among random effects

$$\begin{pmatrix} {U}_{0i} \\ {U}_{1i} \end{pmatrix} \sim \mathcal{N} \begin{pmatrix} 0, & \tau_{00}^{2} & \tau_{01}\\ 0, & \tau_{01} & \tau_{10}^{2} \end{pmatrix}$$

The variances and the covariation (correlations) can be of substantive interest. What do each of these terms reflect? What if one of the terms was zero, what would that mean?

## residual

$$ {\varepsilon}_{ti} \sim \mathcal{N}(0, \sigma^{2})  $$ Much like in normal regression models we often use $\sigma^{2}$ as a means to describe the fit of the model

------------------------------------------------------------------------

![](5.3.png)

## model comparisons

In setting up the basic growth model we have a series of questions to address:

1.  Do we need to add a time component?
2.  If so, do we need to allow that to vary across people?
3.  if so, do we want to allow the intercept to correlate with the slope?

Usually 1 & 2 are explicitly tested whereas 3 is more theoretical

------------------------------------------------------------------------

![](5.1.png)

## centering redux

The correlation among random intercept and slopes is directly related to centering of variables. The two standard choices for time is to center at the mean of time or at the start of time. Both have their pros and cons.

![](5.4.png){width="550px"}


## Other types of models


Depending on your DV, you might not want to have a Gaussian sampling distribution. Instead you may want something like a Poisson or a negative binomial if you are using some sort of count data. You can do this somewhat with lme4. However, the BRMS package -- which uses Bayesian estimation -- has many more possibilities: geometric, log normal, weibull, exponential, gamma, Beta, hurdle Poisson/gamma/negative binomial, zero inflated beta/Poisson/negative binomial, cumulative. Maybe we will fit some of these later in the semester. 



## Accelerated longitudinal designs

- Want to track the trend of a construct across the lifespan, using data that do not cover an entire lifespan (eg. made up of people who have some longitudinal data)

- Time needs to be set as age. Cannot do time since baseline as our time metric. 

-----------------


```{r}
#| code-fold: true
set.seed(123)

n_per_cohort <- 100
time_points <- 0:4  

df <- expand.grid(
  ID_in_Cohort = 1:n_per_cohort,
  Cohort_Start_Age = c(20, 30, 40, 50), 
  Time = time_points                
)

df$ID <- paste0(df$Cohort_Start_Age, "_", df$ID_in_Cohort)

df$Outcome <- (df$Cohort_Start_Age * 1.5) + (df$Time * 0.5) + rnorm(nrow(df))


acc.model <- lmer(Outcome ~ Time * Cohort_Start_Age + (Time | ID), data = df)

summary(acc.model)
```


-------------


```{r}
#| code-fold: true
pred_grid <- expand.grid(
  Cohort_Start_Age = c(20, 30, 40, 50),  
  Time = seq(0, 4, by = 0.5)         
)

pred_grid$Chronological_Age <- pred_grid$Cohort_Start_Age + pred_grid$Time


pred_grid$Predicted_Outcome <- predict(acc.model, newdata = pred_grid, re.form = NA)

ggplot(pred_grid, aes(x = Chronological_Age, 
                      y = Predicted_Outcome, 
                      group = factor(Cohort_Start_Age), 
                      color = factor(Cohort_Start_Age))) +
  geom_line(size = 2) +
  stat_smooth(aes(group = 1), method = "lm", 
              color = "grey", se = FALSE, size = 0.5) +
  labs(title = "Cohort-Sequential Plot",
       x = "Chronological Age",
       y = "Outcome Score",
       color = "Cohort (Start Age)") +
  theme_minimal() +
  scale_color_brewer(palette = "Dark2")
```




## Fixed effects

Other fields model dependencies differently. Econometrics for example often uses "fixed effects" for panel studies where you add Person ID as a categorical predictor rather than account for dependencies via random effects. 

While it adequately controls for person dependencies it does not allow prediction of why individual differences occurred


## Multiple level 1 predictors

Level 1:

$${Y}_{ti} = \beta_{0i}  + \beta_{1i}time_{ti} + \beta_{2i}Z_{ti} + \varepsilon_{ti}$$ 
Level 2:
$${\beta}_{0i} = \gamma_{00} + U_{0i}$$

$${\beta}_{1i} = \gamma_{10} + U_{1i}$$ 
$${\beta}_{2i} = \gamma_{20} + U_{2i}$$

-------

- Remember that MLM is just like a lot of separate regressions done at Level 1. 

- What happens when we have multiple predictors? The effect of any one variable partials out the common variance, the same as with multiple regression. 






## Level 2 predictors

Level 1: $${Y}_{ti} = \beta_{0i}  + \beta_{1j}time_{ti} + \varepsilon_{ti}$$ 

Level 2: $${\beta}_{0i} = \gamma_{00} + \gamma_{01}G_{i} +   U_{0i}$$

$${\beta}_{1i} = \gamma_{10} + U_{1i}$$

-------


Combined $${Y}_{ti} = \gamma_{00} + \gamma_{01}G_{i}+  \gamma_{10} Time_{ti} + U_{0i} + U_{1j}Time_{ti} + \varepsilon_{ti}$$

$${Y}_{ti} = [\gamma_{00} + \gamma_{01}G_{i}+ U_{0i}]  + [(\gamma_{10}  + U_{1i})(Time_{ti})] + \varepsilon_{ti}$$

------------

-   Level 2 refers to Between-Person (BP) Variation, INTER-individual, Differences, Time-Invariant, make comparisons across individuals.

- Some variables can be both depending on how you measure them. E.g. Age

- Level 1 and Level 2 can be differentiated in your dataset by which one repeats vs which is consistent. 



## Cross level interactions

level 1: $${Y}_{ti} = \beta_{0i}  + \beta_{1i}time_{ti} + \varepsilon_{ti}$$ Level 2: $${\beta}_{0i} = \gamma_{00} + \gamma_{01}G_{i} +   U_{0i}$$

$${\beta}_{1i} = \gamma_{10} + \gamma_{11}G_{i} + U_{1i}$$

$${Y}_{ti} = \gamma_{00} + \gamma_{01}G_{i}+  \gamma_{10} (time_{ti}) + \gamma_{11}(G_{i}*time_{ti}) +  U_{0j} + U_{1i}(time_{ti}) + \varepsilon_{ti}$$

$${Y}_{ti} = [\gamma_{00} + U_{0i} +\gamma_{01}G_{i}] + [(\gamma_{10}  + \gamma_{11}G_{i}+  U_{1i})(time_{ti})] + \varepsilon_{ti}$$




