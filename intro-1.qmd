---
title: intro
format: revealjs
slide-number: true
editor: visual
execute:
  echo: true
html:
  code-fold: true
  code-summary: Show the code
---

## Goals for today/week:

```{r, echo = FALSE}

library(tidyverse)
library(readr)
library(viridis)
library(rmarkdown)
```

-   Get a feeling for how to think about longitudinal/repeated measures data. 

-   Begin to develop a framework for analysis

-   Introduce some important terms and methodological tools

 
## how to think longitudinal-y

- Scope of class: 

1. Cover basic types of statistical models: MLM/SEM/GAM

2. Charting change linearly, then nonlinearly, then adding predictors to understand that change 

3. Use these building blocks to extend to multivariate (change with change) models 

## Common questions

- Does variable Y changes across time/age/wave/year? (If so, what is shape?)
- Does every person change the same on Y? 
- How does person Q change on variable Y? 
- Does experience X change Y? (If so, are these changes lasting?)
- Does variable X change with variable Y? 
- Does change in X precede change in Y? 

## What do we mean by change? 

- How has your height changed? If you're tall, where you always tall? 

- How has your level of sleepy-ness changed? 

## Key things to keep in mind

- In reference to what; what is the backdrop we are describing something as "different"? (Absolute vs relative measurement)

- Time scale; speed of change (can we see change over year or does it need to be measured each day or hour?)

- Are our measures sensitive to change? (Tall vs not tall compared to measuring inches/cm)

- Change vs fluctuation (is there a setpoint that we want to see deviations from vs a change in baseline?)

## Inter vs intra

-   Between-Person (BP) Variation, Level-2, INTER-individual differences, Time-Invariant, make comparisons to other people.

-   Within-Person (WP) Variation, Level-1, INTRA-individual, Differences, Time-Varying, make comparisons to an individual.

- Often interested in "individual differences in intraindividual" change

## Terms

-   Within-Person Change is often referred to as *systematic* (relatively lasting) change. Can refer to between-person (inter-individual) differences in within-person change (intra-individual)

-   Within-Person Fluctuation is *not systematic* change. The process varies/fluctuates over time (e.g., emotion, stress) and is not due to error in measurement. Often time is in the model, but CHANGE is not the focus.

----------

```{r}
#| code-fold: true
simp<- tribble(
  ~ID,  ~Y, ~time,
1,5,1,
1,7,2,
2,4,1,
2,6,2,
3,3,1,
3,5,2,
4,2,1,
4,4,2,
5,1,1,
5,3,2)

ggplot(simp, aes(x=time, y=Y)) +
    geom_point(size=5) + theme(text = element_text(size = 20)) 
```



## How do we define "change"?

Types of change (most common):

-   Differential / rank order consistency/rank order stability (correlations)

-   Mean level/ absolute change (mean differences)

------------------------------------------------------------------------

::::: columns
::: {.column width="50%"}
Perfect rank order, mean level increase
:::

::: {.column width="50%"}
No rank order, mean level increase
:::
:::::

```{r}
#| code-fold: true
library(tidyverse)

 ro.ml <- ggplot(simp, aes(x=time, y=Y)) +
    geom_point(size=5) + 
  stat_summary(fun = mean, geom="line", size = 4)+ geom_smooth(aes(group = ID), method=lm, se=FALSE) + theme(text = element_text(size = 20))

simp2<- tribble(
  ~ID,  ~Y, ~time,
1,1,1,
1,5,2,
2,1.5,1,
2,4.5,2,
3,2,1,
3,4,2,
4,2.5,1,
4,3.5,2,
5,3,1,
5,3,2)

noro.ml<- ggplot(simp2, aes(x=time, y=Y)) +
    geom_point(size=5) +   stat_summary(fun = mean, geom="line", size = 4) + geom_smooth(aes(group = ID), method=lm, se=FALSE) + theme(text = element_text(linewidth = 20))

library(patchwork)
ro.ml + noro.ml

```

## How do we define "change"?

-   Because there are many types of change, we will view change in terms of the model.

-   (Usually) it is clearer to refer to the type of change in terms of an equation or pictorially. 

-   Models may be able to tell us about two different types of change (within person vs between person change) and two or more models may be capable of addressing the same type of change

## Ideal prerequisites

-   Ordinal or greater scale of measurement us easiest. Dichotomous is hard.

-   Construct has the same meaning across measurement occasions. Usually the same items. Called measurement invariance. Complicates developmental/child work.

-   2 or more measurement occasions. More is better! Though often 3 - 10 is practically fine for some models. With 30+ occasions you have "intensive" longitudinal data which presents new models and opportunities.

## Example

::: panel-tabset

### Data

```{r, message=FALSE, warning = FALSE}
#| code-fold: true
example <- read_csv("https://raw.githubusercontent.com/josh-jackson/longitudinal-2021/master/example.csv")
example$year <- example$week
example 
```

### Plot

```{r, echo = TRUE}
#| code-fold: true
ggplot(example,
   aes(x = year, y = SMN7, group = ID)) + geom_point()  

```
:::

------------------------------------------------------------------------

To begin, always think of our time metric on the X-axis


---------------

```{r, echo = TRUE}
#| code-fold: true
ggplot(example,
   aes(x = year, y = SMN7, group = ID, colour = ID)) + geom_line(alpha = .4) +scale_color_viridis(   ) 
```

## Individual level

```{r, echo = TRUE, warning = FALSE, message=FALSE}
#| code-fold: true
ggplot(example, aes(x = year, y = SMN7, group = ID)) + geom_line() +  geom_point() + facet_wrap( ~ ID)

```

## Two key questions we need to answer

1.  Shape. What is the theoretical shape we want to model - linear, quadratic, cyclical, etc?

2.  Time. Is shape related to calendar time, age, or maybe artificial time such as grade?

## Linear shape

- Modeling linear change as a first step is great: 

Easy to interpret; less likely to overfit; data requirements are higher for more complex models -- maybe only capable of seeing linear; underfitting isn't horrible for social science theories

## Defining a time metric

-   Time is the most important part of a longitudinal analyses. The key to interpreting your output is to know how you handled your time variable.

-   What is the process that is changing someone? Age? Time in study? Year? Wave?

-   Is it a naturally occurring developmental process? Then maybe age is the best metric. What about tracking child's cognitive ability, something that might be influenced by level of schooling? 

## Defining a time metric

-   There is no right time metric. A time metric is merely how you interpret the lines you are creating.

-   However, your choice will determine what research questions you can answer. E.g. do different age groups change differently? Age cannot be your time metric because your question implies a different time metric by which age moderates.

## Statistical model

-   With a theoretical model of change in mind, and a good temporal design, we can then choose our statistical model.

-   This matching of theory with design with a model is similar to all of stats. We will be using three general purpose models that are related, but have pros and cons in different areas: MLM, SEM, and GAMs


-------------

Briefly, pros: 

- MLM: continuous time, missing data 
- SEM: measurement models, multivariate 
- GAM: nonlinear change


## Why not RM ANOVA?

1.  Cannot handle missing data (listwise)
2.  Assumes rate of change is the same for all individuals.
3.  Time is treated as categorical.
4.  Accounting for correlation across time uses up many parameters (df penalty).
5.  Cannot handle some types of predictors
6.  Is a special case of MLM, so might as well learn/use flexible model



## Simple to begin with

Before we get too fancy, lets just run some regressions.

```{r}
#| code-fold: true
set.seed(11)
ex.random <- example %>% 
  dplyr::select(ID) %>% 
  distinct %>% 
  sample_n(10) 

example2 <-
  left_join(ex.random, example)  
  
ggplot(example2,
   aes(x = week, y = SMN7, group = ID)) +  geom_point() + stat_smooth(method="lm", se = FALSE) + facet_wrap( ~ID)
```

## Individual regression output

```{r, echo = TRUE}
#| code-fold: true
library(broom)
regressions <- example2 %>% 
  group_by(ID) %>% 
  do(tidy(lm(SMN7 ~ week, data=.)))

regressions
```

## Average each regression

```{r, echo = TRUE}
regressions %>% 
  group_by(term) %>% 
  summarise(avg.reg = mean(estimate))
```

This is not that far off from what MLM gives us.

## Spaghetti Plot

```{r}
#| code-fold: true
ggplot(example, aes(x = year, y = SMN7, group = ID)) + geom_point(alpha = 0.05) + stat_smooth(method = "lm", se = FALSE) +  stat_smooth(data = example, aes(x = year, y = SMN7, group=1, color = "black"), method = "lm", size = 2) + guides(fill="none")+ theme(legend.position="none")

```

-----------

~~ Building intuitions ~~ 


## 2 time point models

-   Many designs only have 2 time points of data i.e. pre/post.

-   These types of designs have been derided for decades  such that we shouldn't even look at change because they have too many problems (Cronbach & Furby, 1970; Rogosa, 1983).

-   Problems include: hard to separate measurement error from true change, unreliable estimate of change, change is often associated with initial level

## Simple regression models

That said, it is useful to think through how one would analyze 2 timepoint data.

If you had pre-post data, what would your linear regression model look like? Two options. 

```{r}
#| code-fold: true
prepost <- example %>% 
  select(ID, group, wave, SAL) %>% 
  pivot_wider(names_from = wave, values_from = SAL, names_prefix = "T") %>% 
  mutate(pre = T1) %>% 
  mutate(post = T2) %>% 
  select(ID, pre, post, group)
  prepost
```

## Change/difference score

One large category of models are referred to as change score, because your DV is a computed difference (or change) score.

```{r}
M1 <- lm((post - pre) ~ 1, data = prepost)
tidy(M1)
```

------------------------------------------------------------------------

+
  PROs:  
  
  Gives average change directly. 
  
- CONS:

  Separate computation outside model. 
  
  Doesn't scale well. 
  
  Assumes perfect change. 
  
  Initial (or last) assessment often highly correlated with change

## residualized (autoregressive) model

- Residual is often used as a measure of change. "What is left over after accounting for T1" 

```{r}
M2 <- lm(post ~ 1 + pre, data = prepost)
tidy(M2)
```


--------

+ PROs:

  Accounts for error by regressing to the mean, almost "pooling" towards the average as in MLMs.  
  
 Controls for initial level. 
  
- CONs: 

  Assumes people change via the same process. 
  
  Not a principled way to account for measurement error.
  
  Assumes same reliability for all. 


------------

```{r}
#| code-fold: true
M1.d <- augment(M1) |> 
  rename(diff = '(post - pre)')

M2.d <- augment(M2) |> 
  select(.resid) |> 
  transmute(res.change = .resid)

cbind(M1.d, M2.d) |> 
  select(diff, .resid, res.change) |> 
  cor()
```


------------------------------------------------------------------------

But how much did people change, mean level wise?

```{r}
#| code-fold: true
p2 <- predict(M2)
predict.m2 <- cbind(prepost,p2 )
 predict.m2 %>% 
  mutate(res.change = p2 - pre) %>% 
  summarise(mean(res.change))
```

- The two approaches can be similar, but they don't have to be (Lord's paradox). Depends on if they start at similar places (who changed the most vs who ended up higher than expected, given where they started). 

------------------------------------------------------------------------

What is not immediately obvious is that the change (difference) score model can be conceptualized as a specific type of residualized change score model

`T2 = b*T1 + e`

If we assume that the relationship (b) between T1 and T2 is 1. 

`T2 = 1*T1 + e`

Then we can subtract T1 from each side of the model, leaving:

`T2 - T1 = e`

A change score is equivalent to assuming a perfect  association between timepoints. No regression to the mean, and thus no accounting for starting differences. 

------------------------------------------------------------------------


```{r}
#| code-fold: true
set.seed(555) 
n <- 500    
df <- data.frame(
  group = c(rep("Control", n), rep("Tx", n)),
  t1 = c(rnorm(n, mean = 90, sd = 10), rnorm(n, mean = 50, sd = 10))
)

df$group <- as.factor(df$group)
df$t2 <- 20 + (0.8 * df$t1) + rnorm(2*n, mean=0, sd=5)

df$change_score <- df$t2 - df$t1

mod_change <- lm(change_score ~ group, data = df)

mod_resid <- lm(t2 ~ t1 + group, data = df)

df %>%
  group_by(group) %>%
  summarise(across(c(t1, t2,change_score), mean)) 
```



----------------

```{r}
#| code-fold: true
tidy(mod_change)
```

```{r}
#| code-fold: true
tidy(mod_resid)
```


-------

Difference score tx increases relative compared to control
```{r}
#| code-fold: true
ggplot(df, aes(x = t1, y = t2, color = group)) +
  geom_point(alpha = 0.5, size = 2) +
  geom_abline(aes(intercept = 0, slope = 1, 
                  linetype = "Change Model"), 
              color = "black", size = 1) +
  
  # geom_smooth(method = "lm", se = FALSE, 
              # aes(group = 1, linetype = "Residualized"), 
              # color = "black", size = 1.2) +
  
  scale_color_manual(values = c("Tx" = "blue", 
                                "Control" = "green")) +
  scale_linetype_manual(name = "Model Assumptions",
                        values = c("Change Model" = "dashed", 
                                   "Residualized" = "solid")) +
  labs(title = "Visualizing Lord's Paradox",
       x = "Time 1 (Baseline)",
       y = "Time 2 (Follow-up)") +
  theme_minimal() 
```


-------

Residual controls for initial values, looking at changes assuming everyone starts the same. 

```{r}
#| code-fold: true
ggplot(df, aes(x = t1, y = t2, color = group)) +
  geom_point(alpha = 0.5, size = 2) +
  geom_abline(aes(intercept = 0, slope = 1, 
                  linetype = "Change Model"), 
              color = "black", size = 1) +
  geom_smooth(method = "lm", se = FALSE, 
              aes(group = 1, linetype = "Residualized"), 
              color = "black", size = 1.2) +
  scale_color_manual(values = c("Tx" = "blue", 
                                "Control" = "green")) +
  scale_linetype_manual(name = "Model Assumptions",
                        values = c("Change Model" = "dashed", 
                                   "Residualized" = "solid")) +
  labs(title = "Visualizing Lord's Paradox",
       x = "Time 1 (Baseline)",
       y = "Time 2 (Follow-up)") +
  theme_minimal() +
  theme(legend.position = "bottom",
        text = element_text(size = 12))
```

## 2 timepoint summary

-   Approaches differ on how much we care about accounting for T1 differences. 

- What we want to do is create a model that is flexible enough for us to implement this assumption (and address others -- no measurement error). 

- One type of change can be equivalent to another, depending 

- Both of these models are not easily adapted to more than 2 waves and or for use with multivariate questions. We need a new approach. 



--------------------------------------------------


~~~ Intermission ~~~




## DATA

- Why are we talking about data? Because 80%, maybe more, of your time spent with "analysis" is spent getting data in order and setting up your model of interest, especially true for longitudinal data.

- Your AI overloads can do a lot of the heavy lifting, but it is helpful to understand the logic behind what the functions are doing and hammer home terminology. 

## Wide vs long

-   multivariate vs stacked

-   person vs person period

-   untidy vs tidy

-   Long is what MLM, ggplot2 and tidyverse packages expect whereas SEM and a lot of descriptives are calculated using wide dataframes.

------------------------------------------------------------------------

![](tidyr.gif)

## tidyr pivot functions

For longitudinal/repeated measures data, each row is an observation. Each person will have multiple rows.

```{r}
long<-example %>% 
  select(ID, wave, group, DAN)
long
```

## pivot_wider

The pivot_wider() function takes two arguments: names_from which is the variable whose values will be converted to column names and values_from whose values will be cell values.

```{r, echo =TRUE}
wide.ex <- long %>% 
  pivot_wider(names_from = wave, values_from = DAN) 
wide.ex
```

------------------------------------------------------------------------

```{r, echo =TRUE}
wide.ex <- long %>% 
  pivot_wider(names_from = wave, values_from = DAN, names_prefix = "T") 
wide.ex
```

## pivot_longer

The pivot_longer function takes three arguments: 

- cols is a list of columns that are to be collapsed. The columns can be referenced by column number or column name. 

- names_to is the name of the new column which will combine all column names. This is up to you to decide what the name is. 

- values_to is the name of the new column which will combine all column values associated with each variable combination.

------------------------------------------------------------------------

```{r, echo = TRUE}
wide.ex %>% 
  pivot_longer(cols = 'T1':'T4', 
               names_to = "wave", 
               values_to = "DAN")
```

## Seperate and Unite

-   Many times datasets are, for a lack of a better term, messy.

-   One common way to represent longitudinal data is to name the variable with a wave signifier.

```{r}
wide<- tribble(
  ~ID, ~ext_1, ~ext_2, ~ext_3,
  1, 4, 4,4,
  2, 6, 5,4,
  3, 4,5,6
)
wide
```

------------------------------------------------------------------------

```{r, echo = TRUE}
wide %>% 
  pivot_longer(cols = ext_1:ext_3, names_to = "time", values_to = "EXT")
```

. . .

The time column is now specific to ext, which is a problem if I have more than one variable that I am pivoting. So how can we go ahead and separate out the ext part?

------------------------------------------------------------------------

```{r, echo = TRUE}
#| code-fold: true
wide %>% 
  pivot_longer(cols = ext_1:ext_3, 
               names_to = "time", 
               values_to = "EXT") %>% 
  separate(time, into = c("variable", "time"))

```

------------------------------------------------------------------------

In terms of setting up your data, it is often helpful to include markers that separate parts of the variable eg "\_" or "." A variable that is ext_1 is easier to separate than ext1.

Also that the time column is a character rather than numeric. We need to change this so as to use time continuously in our models. There are a few ways to do it, but this is perhaps the most straightforward.

```{r, eval = FALSE, echo=TRUE}
long$time <- as.numeric(long$time)
```

------------------------------------------------------------------------

Something that is a little more elegant is to do both the separating AND the making into numeric in the original pivot_longer function

names_prefix omits what is in there from the new cell names. Previously we had ext_1, ext_2, etc, which we had to separate with a different function, but this does it within pivot_longer

```{r, echo = TRUE, eval = FALSE}
wide %>% 
  pivot_longer(cols = ext_1:ext_3, 
               names_to = "time", 
               values_to = "EXT", 
               names_prefix = "ext_") 
```

------------------------------------------------------------------------

names_transform does any transformations within the variables. Here instead of a separate call, we can make our variables numeric.

```{r, echo = TRUE}
wide %>% 
  pivot_longer(cols = ext_1:ext_3, 
               names_to = "time", 
               values_to = "EXT", 
               names_prefix = "ext_", 
               names_transform = list(time = as.numeric)) 
```

------------------------------------------------------------------------

Another common problem that we often face is the need to unite two variables into one. Enter, the creatively titled unite function. Sometimes this happens when our time metric is entered in separate columns.

```{r}
df<-tibble(
      ID = c(1,   2,  3),
      year  = c(2020,  2020, 2020),
      month  = c(1,  1, 1),
      day  = c(1,  1, 1),
      hour   = c(4,  2, 5),
      min   = c(55, 17, 23))
df
```

------------------------------------------------------------------------

To combine them into one time metric

```{r, echo= TRUE}
#| code-fold: true
df %>% 
  unite(col = time, 5:6, sep=":", remove =TRUE)
```

## Date time metrics

```{r, echo=TRUE}
library(lubridate)
```

A date-time is a date plus a time: it uniquely identifies an instant in time (typically to the nearest second). These are called POSIXct in R.

```{r, echo = TRUE}
today()
```

```{r, echo = TRUE}
now()
```

------------------------------------------------------------------------

Bringing these into R from some outside place (excel, spss) can lead to confusion, as they can be formatted differently

```{r, echo = TRUE}
ymd("2017-01-31")

mdy("January 31st, 2017")

dmy("31-Jan-2017")

```

------------------------------------------------------------------------

You can create these relatively straight forwardly...by hand

```{r, echo = TRUE}
ymd_hms("2017-01-31 20:11:59")

mdy_hm("01/31/2017 08:01")

```

Or you can use existing columns variables. This is where the {lubridate} package comes in handy

------------------------------------------------------------------------

```{r, echo = TRUE}
df %>% 
  mutate(t_1 = make_datetime(year, month, day, hour, min))
```

Note the t_1 variable is a POSIXct (date time) variable type. Once in this format it is much easier to manipulate and work with dates and times.

## Projects and Rmarkdown

As with any project, but especially for longitudinal data, one of the most important aspects of data analysis is A. not losing track of what you did and B. being organized.

1.  rstudio projects 2. git and 3. codebooks are helpful in accomplishing these two goals. We will talk about #1 and #3 but I also encourage you to read about git. These are not the only way to do these sorts of analyses but I feel that exposure to them is helpful, as often in the social sciences these sort of decisions are not discussed.

------------------------------------------------------------------------

The main reason I am going over this is because too much of the code I see looks like this:

```{r, eval = FALSE, echo = TRUE}

mutate(score_mean1 = rowMeans(data[ ,c("Score1a", "Score2a", "Score3a", "Score4a")]))
data <- import %>%
  mutate(score_mean2 = rowMeans(data[ ,c("Score1b", "Score2b", "Score3b", "Score4b")]))
data <- import %>%
  mutate(score_mean3 = rowMeans(data[ ,c("Score1c", "Score2c", "Score3c", "Score4c")]))
data <- import %>%
  mutate(score_mean4 = rowMeans(data[ ,c("Score1d", "Score2d", "Score3d", "Score4d")]))
data <- import %>%
  mutate(score_mean5 = rowMeans(data[ ,c("Score1e", "Score2e", "Score3e", "Score4e")]))
data <- import %>%
  mutate(score_mean6 = rowMeans(data[ ,c("Score1f", "Score2f", "Score3f", "Score4f")]))
data <- import %>%
  mutate(score_mean7 = rowMeans(data[ ,c("Score1g", "Score2g", "Score3g", "Score4g")]))
data <- import %>%
  mutate(score_mean8 = rowMeans(data[ ,c("Score1h", "Score2h", "Score3h", "Score4h")]))
data <- import %>%
  mutate(score_mean9 = rowMeans(data[ ,c("Score1i", "Score2i", "Score3i", "Score4i")]))
```

## Data and code wants/NEEDS:

- Chain of processing where you start with RAW data and end up with the cleaned data. 

- Know what variable names refer to in terms of item content and response scale

- Know how any manipulations were made (composites, reverse scores, omitting values)

- Code that is efficient and documented, using for loops/purrr rather than copy/paste or integrating existing codebooks

------------------------------------------------------------------------

When I create an rmarkdown document for my own research projects, I always start by setting up 3 components:

1.  Packages\
2.  Codebook(s)\
3.  Data

Below, we will step through each of these separately, setting ourselves up to (hopefully) flawlessly communicate with `R` and our data.

## Packages

Packages seems like the most basic step, but it is actually very important. Depending on what gets loaded you might overwrite functions from other packages. (Note: I will often reload or not follow this advice within lectures for didactic reasons, choosing to put library calls above the code)

```{r, echo = TRUE}
# load packages
library(psych)
library(plyr)
library(tidyverse)
```

## Codebook

Arguably, this is the first step because you should *create* the codebook long before you open `R`.

Why a codebook? Because you have a lot of variables and you will not be able to remember all the details that go into each one of them.

It will also allow us to automate some tasks that are somewhat cumbersome and facilitate open data practices. It is a "CODE"book after all. 

------------------------------------------------------------------------

To illustrate, we are going to using some data from the German Socioeconomic Panel Study (GSOEP), which is an ongoing Panel Study in Germany. Note that these data are for teaching purposes only, shared under the license for the Comprehensive SOEP teaching dataset. The full data are available for free at: https://www.diw.de/en/diw_02.c.222829.en/access_and_ordering.html.

------------------------------------------------------------------------

I created the codebook for you, and included what I believe are the core columns you may need.

https://raw.githubusercontent.com/josh-jackson/alda24/main/codebook.csv

Some of these columns will not be particularly helpful for this dataset. For example, many of you likely work with datasets that have only a single file while others work with datasets spread across many files (e.g., different waves, different sources).

------------------------------------------------------------------------

1.  **dataset** this column indexes the **name** of the dataset that you will be pulling the data from.

2.  **old_name** this column is the name of the variable in the data you are pulling it from. This should be exact. The goal of this column is that it will allow us to `select()` variables from the original data file

3.  **item_text** this column is the original text that participants saw or a description of the item.

4.  **scale** this column tells you what the scale of the variable is. Is it a numeric variable, a text variable, etc.

------------------------------------------------------------------------

5.  **reverse** this column tells you whether items in a scale need to be reverse coded. I recommend coding this as 1 (leave alone) and -1 (reverse).

6.  **mini** this column represents the minimum value of scales that are numeric. Leave blank otherwise.

7.  **maxi** this column represents the maximum value of scales that are numeric. Leave blank otherwise.

8.  **recode** sometimes, we want to recode variables for analyses (e.g. for categorical variables with many levels where sample sizes for some levels are too small to actually do anything with it). I use this column to note the kind of recoding I'll do to a variable for transparency.

------------------------------------------------------------------------

Here are additional columns that will make our lives easier or are applicable to some but not all data sets:

9.  **category** broad categories that different variables can be put into. I'm a fan of naming them things like "outcome", "predictor", "moderator", "demographic", "procedural", etc. but sometimes use more descriptive labels like "Big 5" to indicate the model from which the measures are derived.

10. **label** label is basically one level lower than category. So if the category is Big 5, the label would be, or example, "A" for Agreeableness. This column is most important and useful when you have multiple items in a scales.

------------------------------------------------------------------------

11. **item_name** This is the lowest level and most descriptive variable. It indicates which item in scale something is. So it may be "kind" for Agreebleness or "sex" for the demographic biological sex variable.

12. **year** for longitudinal data, we have several waves of data and the name of the same item across waves is often different, so it's important to note to which wave an item belongs.

13. **new_name** It's purpose is to be the new name that we will give to the variable that is more useful and descriptive to us. I like to make it a combination of "category", "label", "item_name", and year using varying combos of "\_" and "." that we can use later with tidyverse functions.

------------------------------------------------------------------------

All longitudinal datasets are horrible in some way. Doing this makes them less horrible. Is it some upfront work? Yes. Will it ultimately save you time? Yes. Also, if you know this prior to running a study you are making some sort of code book anyways, right? Right?!? Might as well kill two birds with one stone.

You can make the codebook anyway you want, but the two best options are miscrosoft excel and google pages. Not because they are necessarily the best functioning but because they are relatively ubiquitous and are easy to share.

------------------------------------------------------------------------

We will create a codebook but then bring the codebook into R via turning it into a csv. You can rethink the codebook as a way of coding prior to putting anything into R.

Below, I'll load in the codebook we will use for this study, which will include all of the above columns.

```{r, echo = TRUE}
#| code-fold: true

codebook <- read.csv("https://raw.githubusercontent.com/josh-jackson/alda24/main/codebook.csv")

codebook <- codebook %>% 
    mutate(old_name = str_to_lower(old_name)) # converts to lower case

codebook
```

## Data

Notice the horrible variable names

```{r}
#| code-fold: true
old.names <- codebook$old_name # get old column names
new.names <- codebook$new_name # get new column names

soep <- read.csv("https://raw.githubusercontent.com/josh-jackson/longitudinal-2021/master/soepdata.csv")
soep
```

------------------------------------------------------------------------

```{r}
#| code-fold: true
 soep <-  soep %>% 
  dplyr::select(all_of(old.names)) %>% # select the columns from our codebook
  setNames(new.names) # rename columns with our new names
soep

```

## Recode Variables

Within R we treat missing values as `NA`, but many other programs from which you may be importing your data may use other codes (e.g. 999, -999, etc.).

In the GSOEP, `-1` to `-7` indicate various types of missing values, so we will recode these to `NA`. To do this, we will use `mapvalues()`, from the `plyr` package.

------------------------------------------------------------------------

(1) the variable you are recoding. Below that is indicated by "." which is shorthand for the data that was piped in.

(2) a vector of initial values `from` which you want to change. Here we indicae a sequence of values from -1 to -7, which correspond to the missing values used by GSOEP. Other datasets may use -999, for example.

(3) recode your values in (2) `to` new values in the same order as the old values. Here we have NA (the way R treats missing data) repeated 7 times (to correspond to -1, -2,...)

It is also helpful to turn off warnings if some levels are not in your data (`warn_missing = F`).

------------------------------------------------------------------------

```{r}
#| code-fold: true
library(plyr)
soep <- soep %>%
  mutate_all(~as.numeric(mapvalues(., from = seq(-1,-7, -1), to = rep(NA, 7), warn_missing = F)))
head(soep)
```

------------------------------------------------------------------------

mapvalues technically is depreciated and from an old package {plyr}. The replacement for it is dplyr's recode. However, recode does not work well with vectorized names (which we pull in from our codebook)

and again, this isnt the way you NEED to clean your data. Seeing alternative options help expands your possibilities. ... like study abroad or dating.

## Reverse-Scoring

Many scales we use have items that are positively or negatively keyed. To create the composite scores of a construct, we must first "reverse" the negatively keyed items so that high scores indicate being higher on the construct.

There are a few ways to do this in `R`. Below, I'll demonstrate how to do so using the `reverse.code()` function in the `psych` package in `R`. This function was built to make reverse coding more efficient (i.e. please don't run every item that needs to be recoded with separate lines of code!!).

Before we can do that, though, we need to restructure the data a bit in order to bring in the reverse coding information from our codebook.

------------------------------------------------------------------------

Bring the wide dataset to long

```{r, echo = TRUE}
#| code-fold: true
soep_long <- soep %>%
  pivot_longer(cols = c(-contains("Procedural"), -contains("Demographic")),
               names_to = "item", 
               values_to = "value", 
               values_drop_na = TRUE)
str(soep_long) 
```

```{r}
head(soep_long) 
```

------------------------------------------------------------------------

Bring in the codebook relevant items for reverse coding

```{r}
#| code-fold: true
soep_long <- soep %>%
  pivot_longer(cols = c(-contains("Procedural"), -contains("Demographic")),
               names_to = "item", 
               values_to = "value", 
               values_drop_na = TRUE) %>%
  left_join(codebook %>% select(item = new_name, reverse, mini, maxi)) 
head(soep_long)
```

------------------------------------------------------------------------

Here we want to break our item column up into different components to assist with different calculations. Often you will have some sort of hierarchy of variables where items are nested within scales which are nested within questionnaires.

```{r, echo =TRUE}
#| code-fold: true
soep_long <- soep %>%
  pivot_longer(cols = c(-contains("Procedural"), -contains("Demographic")),
               names_to = "item", 
               values_to = "value", 
               values_drop_na = TRUE) %>%
  left_join(codebook %>% select(item = new_name, reverse, mini, maxi)) %>% 
  separate(item, c("type", "item"), sep = "__") %>% 
  separate(item, c("item", "year"), sep = "[.]") %>% 
  separate(item, c("trait", "item"), sep = "_") 
head(soep_long)
## almost half a million rows
```

------------------------------------------------------------------------

now it is ready to reverse code!

```{r}
#| code-fold: true
soep_long <- soep %>%
  pivot_longer(cols = c(-contains("Procedural"), -contains("Demographic")),
               names_to = "item", 
               values_to = "value", 
               values_drop_na = TRUE) %>%
  left_join(codebook %>% select(item = new_name, reverse, mini, maxi)) %>% 
  separate(item, c("type", "item"), sep = "__") %>% 
  separate(item, c("item", "year"), sep = "[.]") %>% 
  separate(item, c("trait", "item"), sep = "_") %>% 
  mutate(value = as.numeric(value), # change to numeric
         value = ifelse(reverse == -1, 
            reverse.code(-1, value, mini = mini, maxi = maxi), value))
head(soep_long)
```

## Create Composites

Now that we have reverse coded our items, we can create composites.

We'll start with our scale -- in this case, the Big 5.

The "simplest" way, which is also the longest and most prone to error way because you'd have to do it separately for each scale, in each year is to use a function like rowMeans which I don't recommend as that will be MANY MANY lines of code.

------------------------------------------------------------------------

```{r, echo = TRUE, eval = FALSE}

# this but for each wave by each construct times

soep$C.2005 <- with(soep, rowMeans(cbind(`Big 5__C_thorough.2005`, 
`Big 5__C_lazy.2005`, 
`Big 5__C_efficient.2005`), 
na.rm = T)) 

soep$C.2009 <- with(soep, rowMeans(cbind(`Big 5__C_thorough.2009`, 
`Big 5__C_lazy.2009`, 
`Big 5__C_efficient.2009`), 
na.rm = T)) 

soep$C.2013 <- with(soep, rowMeans(cbind(`Big 5__C_thorough.2013`, 
`Big 5__C_lazy.2013`, 
`Big 5__C_efficient.2013`), 
na.rm = T)) 

soep$C.2013 <- with(soep, rowMeans(cbind(`Big 5__C_thorough.2021`, 
`Big 5__C_lazy.2021`, 
`Big 5__C_efficient.2012`), 
na.rm = T)) 

```

DO NOT COPY AND PASTE IN YOUR CODE

------------------------------------------------------------------------

We can use our codebook and `dplyr` to make our lives a whole lot easier. In general, trying to run everything simultaneously saves from copy-paste errors, makes your code more readable, and reduces the total amount of code. So while the below code may not make intuitive sense immediately, it is nonetheless what we are working towards. Also, going through line by line will help you see that.

First, make sure we are only working with Big Five rows.

```{r}
#| code-fold: true
b5_soep_long <- soep_long %>%
  filter(type == "Big 5")
str(b5_soep_long)
```

------------------------------------------------------------------------

```{r, echo=TRUE}
#| code-fold: true
b5_soep_long<- soep_long %>%
  filter(type == "Big 5") %>% 
  group_by(Procedural__SID, trait, year) %>% 
  dplyr::summarize(value = mean(value, na.rm = T)) 
head(b5_soep_long)
```

------------------------------------------------------------------------

Now that we have our means we can bring the demographic info back into the dataframe...or whatever else you would want to bring in.

```{r, echo = TRUE}
#| code-fold: true
b5_soep_long <- soep_long %>%
  filter(type == "Big 5") %>% 
  group_by(Procedural__SID, trait, year) %>% 
  dplyr::summarize(value = mean(value, na.rm = T)) %>% 
  ungroup() %>% 
  left_join(soep_long %>% 
    select(Procedural__SID, DOB = Demographic__DOB, Sex = Demographic__Sex) %>%
    distinct())

head(b5_soep_long) 
```

## Descriptives

Descriptives of your data are incredibly important. They help you check the distribution of your variables (e.g. non-normally distributed), look for implausible values made through coding or participant error, and allow you to anticipate what your findings will look like.

## metric variables

```{r, echo = TRUE}
#| code-fold: true
library(psych)
b5_soep_long_des <- b5_soep_long %>%
  unite(tmp, trait, year, sep = "_") 
head(b5_soep_long_des)
```

------------------------------------------------------------------------

```{r, echo = TRUE}
#| code-fold: true
b5_soep_long_des <- b5_soep_long %>%
  unite(tmp, trait, year, sep = "_") %>%  
  pivot_wider(names_from = tmp, values_from = value) 
head(b5_soep_long_des)
```

------------------------------------------------------------------------

```{r}
#| code-fold: true
b5_soep_long_des <- b5_soep_long  %>%
  unite(tmp, trait, year, sep = "_") %>% 
  pivot_wider(names_from =tmp, values_from = value) %>% 
  describe(.) 
head(b5_soep_long_des)
```

------------------------------------------------------------------------

```{r}
library(easystats)
b5_soep_long  %>%
  unite(tmp, trait, year, sep = "_") %>% 
  pivot_wider(names_from =tmp, values_from = value) %>% 
  report_table(.) %>% 
  head()
```

## count variables

We have life event variable in the dataset that is a count variable. It asks did someone experience a life event during the previous year. also want to create a variable that indexes whether our participants experienced any of the life events during the years of interest (2005-2015).

```{r}
#| code-fold: true
events_long  <-soep_long %>%
  filter(type == "Life Event") 
head(events_long )
```

------------------------------------------------------------------------

```{r}
events_long <- soep_long %>%
  filter(type == "Life Event") %>% 
  group_by(Procedural__SID, trait) %>% 
  dplyr::summarize(value = sum(value, na.rm = T),value = ifelse(value > 1, 1, 0))
head(events_long )
```

------------------------------------------------------------------------

For count variables, like life events, we need to use something slightly different. We're typically more interested in counts -- in this case, how many people experienced each life event in the 10 years we're considering?

To do this, we'll use a little bit of `dplyr` rather than the base `R` function `table()` that is often used for count data. Instead, we'll use a combination of `group_by()` and `n()` to get the counts by group. In the end, we're left with a nice little table of counts.

------------------------------------------------------------------------

```{r, echo = TRUE}
#| code-fold: true
events_long %>%
  group_by(trait, value) %>% 
  dplyr::summarize(N = n()) 
```

------------------------------------------------------------------------

```{r}
#| code-fold: true
events_long %>%
  group_by(trait, value) %>% 
  dplyr::summarize(N = n()) %>%
  ungroup() %>%
  pivot_wider(names_from = value, values_from = N) 
```

## Kable

```{r}
#| code-fold: true
library(kableExtra)
events_long %>%
  group_by(trait, value) %>% 
  dplyr::summarize(N = n()) %>%
  ungroup() %>%
  pivot_wider(names_from = value, values_from = N, names_prefix = "T") %>% 
  mutate('No Event' = T0) %>% 
  mutate(Event = T1) %>% 
  select(-c(T0, T1)) %>% 
  kbl(booktabs = T) %>% kable_styling(latex_options = "striped")
```

## Zero-Order Correlations

To run the correlations, we will need to have our data in wide format

```{r}
#| code-fold: true
b5_soep_long %>%
  unite(tmp, trait, year, sep = "_") %>%
  pivot_wider(names_from = tmp, values_from = value) %>% 
  select(-Procedural__SID) %>%
  cor(., use = "pairwise") %>%
  round(., 2)

result <- correlation(iris)
```

------------------------------------------------------------------------

```{r}
#| code-fold: true
corrs <- b5_soep_long %>%
  unite(tmp, trait, year, sep = "_") %>%
  pivot_wider(names_from = tmp, values_from = value) %>% 
  select(-Procedural__SID) %>%
  correlation() # from easystat correlation package
corrs
```

------------------------------------------------------------------------

```{r}
plot(summary(corrs))
```

------------------------------------------------------------------------
